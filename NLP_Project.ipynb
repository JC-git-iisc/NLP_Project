{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8ojDCEdYmXq"
      },
      "source": [
        "# Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fr-iQn5Cir6"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
        "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering, DefaultDataCollator, create_optimizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import evaluation_custom as ev\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "import evaluation_custom as ev\n",
        "import json\n",
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1Iwn_s3Yz7y"
      },
      "source": [
        "Default dataset is the SQuAD dataset which is contained in training_set.json\n",
        "and you change it accordingly\n",
        "\n",
        "We can use the model of our choice with *ind* variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8L0igdCXz6C"
      },
      "outputs": [],
      "source": [
        "duorc = False  # Set to False if you want to test only on SQuAD\n",
        "train = False  # Set to False if you do not want to train the model\n",
        "ind = 2  # Change the index from 0 to 2 to test different transformers\n",
        "model_checkpoint_list = [\"distilbert-base-uncased\", \"distilroberta-base\", 'albert-base-v2']\n",
        "model_checkpoint = model_checkpoint_list[ind]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sbj_nLAnqJG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the SQuAD dataset\n",
        "dataset_squad = load_dataset('squad', split='train')  # You can use 'train', 'validation', or 'test' splits\n",
        "df_squad = dataset_squad.to_pandas()\n",
        "\n",
        "# Drop any rows with missing answers (if applicable)\n",
        "df_squad = df_squad.dropna(subset=['answers'])\n",
        "\n",
        "# Initialize lists to store answer start positions and answers\n",
        "start_char = []\n",
        "to_remove = []\n",
        "\n",
        "# Iterate over the rows to find start positions of answers\n",
        "for i, row in df_squad.iterrows():\n",
        "    try:\n",
        "        # Find the start position of the answer in the context\n",
        "        start_char.append(row['context'].index(row['answers']['text'][0]))\n",
        "    except ValueError:  # Handle case where the answer is not found in context\n",
        "        to_remove.append(i)\n",
        "\n",
        "# Drop rows that could not be processed\n",
        "df_squad = df_squad.drop(index=to_remove)\n",
        "\n",
        "# Reset index after removing rows\n",
        "df_squad = df_squad.reset_index(drop=True)\n",
        "\n",
        "# Create answers column as a list of dictionaries\n",
        "df_squad['answers'] = [\n",
        "    {'answer_start': [start], 'text': [ans]}\n",
        "    for start, ans in zip(start_char, df_squad['answers'].apply(lambda x: x['text'][0]))\n",
        "]\n",
        "\n",
        "# Rename columns for consistency\n",
        "df_squad = df_squad.rename(columns={'title': 'title', 'question': 'question', 'context': 'context', 'answers': 'answers'})\n",
        "\n",
        "# Write processed data to a JSON file\n",
        "with open(\"squad.json\", 'w') as f:  # Use 'w' to overwrite the file each time\n",
        "    for i in range(df_squad.shape[0]):\n",
        "        entry = {\n",
        "            \"id\": df_squad['id'][i],\n",
        "            \"title\": df_squad['title'][i],\n",
        "            \"context\": df_squad['context'][i],\n",
        "            \"question\": df_squad['question'][i],\n",
        "            \"answers\": df_squad['answers'][i],\n",
        "        }\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pew1EKafTBba"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset if duorc flag is True\n",
        "if duorc:\n",
        "    dataset_duorc = load_dataset('duorc', 'SelfRC', split='test')\n",
        "    df_duorc = dataset_duorc.to_pandas()\n",
        "\n",
        "    # Drop unnecessary columns and rows\n",
        "    df_duorc = df_duorc.drop(columns=['plot_id', 'no_answer'], errors='ignore')\n",
        "    df_duorc = df_duorc[df_duorc['no_answer'] != True]\n",
        "\n",
        "    # Initialize lists for answer start positions and answers\n",
        "    start_char = []\n",
        "    to_remove = []\n",
        "\n",
        "    # Iterate over the rows to find start positions of answers\n",
        "    for i, row in df_duorc.iterrows():\n",
        "        try:\n",
        "            start_char.append(row['plot'].index(row['answers'][0]))\n",
        "        except ValueError:  # Handle case where the answer is not found in plot\n",
        "            to_remove.append(i)\n",
        "\n",
        "    # Drop rows that could not be processed\n",
        "    df_duorc = df_duorc.drop(index=to_remove)\n",
        "\n",
        "    # Reset index after removing rows\n",
        "    df_duorc = df_duorc.reset_index(drop=True)\n",
        "\n",
        "    # Create answers column as a list of dictionaries\n",
        "    df_duorc['answers'] = [\n",
        "        {'answer_start': [start], 'text': [ans]}\n",
        "        for start, ans in zip(start_char, df_duorc['answers'].apply(lambda x: x[0]))\n",
        "    ]\n",
        "\n",
        "    # Rename columns for consistency\n",
        "    df_duorc = df_duorc.rename(columns={'plot': 'context', 'question_id': 'id'})\n",
        "\n",
        "    # Write processed data to a JSON file\n",
        "    with open(\"duorc.json\", 'w') as f:  # Use 'w' to overwrite the file each time\n",
        "        for i in range(df_duorc.shape[0]):\n",
        "            entry = {\n",
        "                \"id\": df_duorc['id'][i],\n",
        "                \"title\": df_duorc['title'][i],\n",
        "                \"context\": df_duorc['context'][i],\n",
        "                \"question\": df_duorc['question'][i],\n",
        "                \"answers\": df_duorc['answers'][i],\n",
        "            }\n",
        "            f.write(json.dumps(entry) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqmb6HFhZPEc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset from files (train, validation, and test)\n",
        "dataset = load_dataset('json', data_files={'train': train_filename, 'validation': val_filename, 'test': test_filename})\n",
        "\n",
        "# Initialize the ground truth dictionary\n",
        "ground_tr_dict = {}\n",
        "\n",
        "# If the DUORC flag is True\n",
        "if duorc:\n",
        "    # Extract relevant information from the DUORC dataset\n",
        "    ground_truth = []\n",
        "    ids = dataset['test']['id']\n",
        "    answ = dataset['test']['answers']\n",
        "\n",
        "    # Iterate through the test set to extract ground truth data\n",
        "    for i in range(0, len(dataset['test'])):\n",
        "        ground_truth.append([ids[i], answ[i]['text'][0]])\n",
        "\n",
        "    # Save the ground truth data into a JSON file\n",
        "    with open('ground_truth.json', 'w') as json_file:\n",
        "        json.dump(ground_truth, json_file)\n",
        "\n",
        "    # Prepare the final ground truth dictionary\n",
        "    ground_tr_dict['data'] = df_duorc\n",
        "\n",
        "else:\n",
        "    # Prepare ground truth for other datasets (e.g., SQuAD)\n",
        "    ground_tr_dict['data'] = dataset_base['data'][train_split:]\n",
        "    ground_tr_dict['version'] = dataset_base['version'][train_split:]\n",
        "\n",
        "    # Write the ground truth for the other dataset into a JSON file\n",
        "    with open('ground_truth.json', 'w') as json_file:\n",
        "        json.dump(ground_tr_dict, json_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPSyTfOiZdcc",
        "outputId": "25ccb7dc-8ca0-4379-f60d-f71d53991a21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '5733be284776f41900661182',\n",
              " 'title': 'University_of_Notre_Dame',\n",
              " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
              " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
              " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#showing our dataset\n",
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBIzksqI80bE"
      },
      "source": [
        "Now we show the distributions of the lengths of answer, text and question (of training and test set) in order to study the dataframe and the respective results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtSfag9g7-lq"
      },
      "outputs": [],
      "source": [
        "def len_answer(dataset,type):\n",
        "  len_answers = []\n",
        "  for i in range(0,len(dataset[type])):\n",
        "    for j in range(0,len(dataset[type][i]['answers']['text'])):\n",
        "      len_answers.append(len(dataset[type][i]['answers']['text'][j].split()))\n",
        "  return len_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5cilkUf8LoM"
      },
      "outputs": [],
      "source": [
        "def len_maker(dataset,type,feature):\n",
        "  len_ = []\n",
        "  for i in range(0,len(dataset[type])):\n",
        "      len_.append(len(dataset[type][i][feature].split()))\n",
        "  return len_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOJvLb5N8Q2B"
      },
      "outputs": [],
      "source": [
        "# len_answers = len_answer(dataset,\"train\")\n",
        "# keys, counts = np.unique(len_answers, return_counts=True)\n",
        "\n",
        "# plt.figure(figsize=(8,5))\n",
        "# plt.bar(keys, counts)\n",
        "# plt.title('Distribution of Answers length on Training Set', fontsize=16)\n",
        "# plt.ylabel('Number of answer')\n",
        "# plt.xlabel('Answer Length in words')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e-F6E-Z8Uow"
      },
      "outputs": [],
      "source": [
        "# len_answers_val = len_answer(dataset,\"test\")\n",
        "# keys, counts = np.unique(len_answers_val, return_counts=True)\n",
        "\n",
        "# plt.figure(figsize=(8,5))\n",
        "# plt.bar(keys, counts)\n",
        "# plt.title('Distribution of Answers length on Test Set', fontsize=16)\n",
        "# plt.ylabel('Number of answers')\n",
        "# plt.xlabel('Answer Length in words')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "nlodAgmI8gUC",
        "outputId": "1bcca93b-2ce7-4d77-9337-78fbaf9d16f5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "len_question = len_maker(dataset,\"train\", \"question\")\n",
        "keys, counts = np.unique(len_question, return_counts=True)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(keys, counts)\n",
        "plt.title('Question Length Distribution on Training Set', fontsize=16)\n",
        "plt.ylabel('Number of question')\n",
        "plt.xlabel('Question length in words')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4e9DB7I8gXu"
      },
      "outputs": [],
      "source": [
        "len_question_val = len_maker(dataset,\"validation\", \"question\")\n",
        "keys, counts = np.unique(len_question_val, return_counts=True)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(keys, counts)\n",
        "plt.title('Question Length Distribution on Test Set', fontsize=16)\n",
        "plt.ylabel('Number of question')\n",
        "plt.xlabel('Question length in words')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyoAExEdnqJF"
      },
      "outputs": [],
      "source": [
        "batch_size = 2 # suggested by the official paper at 16-32, but due to technical limitation we are forced to set it at 2 and still works well\n",
        "learning_rate = 2e-5 # other admissible by suggestion are 5e-5 and 3e-5\n",
        "num_train_epochs = 3  #Usually for all the 3 models the best number of epochs before starting overfitting\n",
        "weight_decay = 0.01\n",
        "max_length = 384  # The maximum length of a feature (question and context)\n",
        "doc_stride = 128  # The authorized overlap between two part of the context when splitting it is needed.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COV-dRz7nqJI"
      },
      "outputs": [],
      "source": [
        "def tokenize_dataset(dataset):\n",
        "    #Since the roberta-like tokenizer have problems to deal with long sequences of spaces before and after the\n",
        "    #text, it is needed some more data cleaning\n",
        "    if model_checkpoint == \"roberta-base\" or \"distilroberta-base\":\n",
        "        dataset[\"question\"] = [q.lstrip() for q in dataset[\"question\"]]\n",
        "    # Tokenize our questions with truncation and padding, but keep the overflows using a stride. This results\n",
        "    # in one question possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "\n",
        "    tokenized_dataset = tokenizer(  dataset[\"question\"], dataset[\"context\"],\n",
        "                                    truncation=\"only_second\",\n",
        "                                    max_length=max_length,\n",
        "                                    stride=doc_stride,\n",
        "                                    return_overflowing_tokens=True,\n",
        "                                    return_offsets_mapping=True,\n",
        "                                    padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding question. This key gives us just that.\n",
        "\n",
        "    overflowing_tokens = tokenized_dataset.pop(\"overflow_to_sample_mapping\") #with pop we remove the selected column from the dataset and we store it in sample mapping,\n",
        "                                                                         #basically this is a vector with enumerations from 0 to the lenght of the vector\n",
        "                                                                         #created by return_overflowing_tokens in the tokenizer object\n",
        "\n",
        "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "    # help us compute the start_positions and end_positions.\n",
        "\n",
        "    offset_mapping = tokenized_dataset.pop(\"offset_mapping\") # created by return_offsets_mapping in the tokenizer object\n",
        "\n",
        "\n",
        "    # Now we have to label the questions\n",
        "\n",
        "    tokenized_dataset[\"start_positions\"] , tokenized_dataset[\"end_positions\"] = list(),list()\n",
        "\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # We will label impossible answers (the answer is in another feature given by an example with a long context), with the index of the CLS token.\n",
        "        input_ids = tokenized_dataset[\"input_ids\"][i] # The list of numerical token associated to the words of the i-th context\n",
        "\n",
        "        #The first token of every sequence is always a special classification token ([CLS]), in this case is tokenizer.cls_token_id = 101. The final hidden state corresponding to this token is used\n",
        "        # as the aggregate sequence representation for classification tasks. The last token of every sequence is instead 102.\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id) #usually 0\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        # If None, it's a separator (i.e. CLS), 0 is the question tag, 1 is the answer one\n",
        "        sequence_ids = tokenized_dataset.sequence_ids(i)\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = overflowing_tokens[i]\n",
        "        answers = dataset[\"answers\"][sample_index]\n",
        "\n",
        "\n",
        "        # Start/end character index of the answer in the text.\n",
        "\n",
        "        start_char = answers[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "        # Start token index of the current span in the text.\n",
        "        token_start_index = 0\n",
        "        while sequence_ids[token_start_index] != 1:\n",
        "            token_start_index += 1\n",
        "\n",
        "        # End token index of the current span in the text.\n",
        "        token_end_index = len(input_ids) - 1\n",
        "        while sequence_ids[token_end_index] != 1:\n",
        "            token_end_index -= 1\n",
        "\n",
        "        # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "        if(offsets[token_start_index][0] > start_char\n",
        "                or offsets[token_end_index][1] < end_char):\n",
        "\n",
        "            tokenized_dataset[\"start_positions\"].append(cls_index)\n",
        "            tokenized_dataset[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "            # Note: we could go after the last offset if we are in the edge case such that the answer is the last word.\n",
        "            while (\n",
        "                token_start_index < len(offsets)\n",
        "                and offsets[token_start_index][0] <= start_char\n",
        "            ):\n",
        "                token_start_index += 1\n",
        "            tokenized_dataset[\"start_positions\"].append(token_start_index - 1)\n",
        "            while offsets[token_end_index][1] >= end_char:\n",
        "                token_end_index -= 1\n",
        "            tokenized_dataset[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_dataset\n",
        "\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_dataset, batched=True, remove_columns=dataset[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i5mZgmmfN6g"
      },
      "source": [
        "Now we need a data collator that simply collates batches of our processed examples together.\n",
        "\n",
        "Then we can use this data collator to turn our data into a `tf.data.Dataset`, ready for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySxVUoKbnqJJ"
      },
      "outputs": [],
      "source": [
        "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
        "\n",
        "train_set = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"start_positions\", \"end_positions\"],\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=data_collator,)\n",
        "\n",
        "validation_set = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"start_positions\", \"end_positions\"],\n",
        "    shuffle=False,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=data_collator,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi9RzE6MnqJK"
      },
      "outputs": [],
      "source": [
        "total_train_steps = (len(tokenized_datasets[\"train\"]) // batch_size) * num_train_epochs\n",
        "optimizer, schedule = create_optimizer(init_lr=learning_rate, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
        "\n",
        "model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "path = './fine_tuned_' + model_checkpoint +  '/'\n",
        "\n",
        "if train == True:\n",
        "    model.fit(train_set, epochs=num_train_epochs, validation_data=validation_set)\n",
        "    model.save_pretrained(path)\n",
        "else:\n",
        "    model.load_weights(path + 'tf_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wv0vQEl-nqJL"
      },
      "outputs": [],
      "source": [
        "def prepare_test_features(dataset):\n",
        "    #Since the roberta-like tokenizer have problems to deal with long sequences of spaces before and after the\n",
        "    #text, it is needed some more data cleaning\n",
        "    if model_checkpoint == \"roberta-base\" or \"distilroberta-base\":\n",
        "        dataset[\"question\"] = [q.lstrip() for q in dataset[\"question\"]]\n",
        "    # Tokenize our questions with truncation and padding, but keep the overflows using a stride. This results\n",
        "    # in one question possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_dataset = tokenizer(\n",
        "        dataset[\"question\"],\n",
        "        dataset[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    overflowing_tokens = tokenized_dataset.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
        "    tokenized_dataset[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized_dataset[\"input_ids\"])):\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_dataset.sequence_ids(i)\n",
        "        context_index = 1\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        token_index = overflowing_tokens[i]\n",
        "        tokenized_dataset[\"example_id\"].append(dataset[\"id\"][token_index])\n",
        "\n",
        "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
        "        # position is part of the context or not.\n",
        "        tokenized_dataset[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else None)\n",
        "            for k, o in enumerate(tokenized_dataset[\"offset_mapping\"][i])]\n",
        "\n",
        "    return tokenized_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJz1lPnjIdYE"
      },
      "source": [
        "Now we can finally prepare the test dataset and"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "672Yg0-bnqJL"
      },
      "outputs": [],
      "source": [
        "test_features = dataset[\"test\"].map(\n",
        "    prepare_test_features,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"test\"].column_names)\n",
        "\n",
        "test_dataset = test_features.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\"],\n",
        "    shuffle=False,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=data_collator)\n",
        "\n",
        "raw_predictions = model.predict(test_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8LJ5ReKnqJM"
      },
      "outputs": [],
      "source": [
        "def postprocess_qa_predictions(\n",
        "    examples,\n",
        "    features,\n",
        "    all_start_logits,\n",
        "    all_end_logits,\n",
        "    n_best_size=20,\n",
        "    max_answer_length=30,\n",
        "):\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    print(\n",
        "        f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\"\n",
        "    )\n",
        "\n",
        "    # Now we need to\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        valid_answers = []\n",
        "\n",
        "        context = example[\"context\"]\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(\n",
        "                tokenizer.cls_token_id\n",
        "            )\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            min_null_score = feature_null_score\n",
        "\n",
        "            # Go through all possibilities for the n_best_size greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[ -1 : -n_best_size - 1 : -1 ].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if (\n",
        "                        end_index < start_index\n",
        "                        or end_index - start_index + 1 > max_answer_length\n",
        "                    ):\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "\n",
        "                        start_char = offset_mapping[start_index][0]\n",
        "                        end_char = offset_mapping[end_index][1]\n",
        "                        valid_answers.append(\n",
        "                            {\n",
        "                                \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                                \"text\": context[start_char:end_char],\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "                    except IndexError:\n",
        "                        start_char = 0\n",
        "                        end_char = 0\n",
        "                        valid_answers.append(\n",
        "                            {\n",
        "                                \"score\": 0,\n",
        "                                \"text\": context[start_char:end_char],\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[\n",
        "                0\n",
        "            ]\n",
        "        else:\n",
        "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "            # failure.\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "\n",
        "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "final_predictions = postprocess_qa_predictions(\n",
        "    dataset[\"test\"],\n",
        "    test_features,\n",
        "    raw_predictions[\"start_logits\"],\n",
        "    raw_predictions[\"end_logits\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AstRc-qAnqJO",
        "outputId": "52cbe73b-b0a4-4ea0-95d8-9fa9dd1544ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"exact\": 64.24044734389562,\n",
            "  \"f1\": 79.99211122229185,\n",
            "  \"total\": 21460,\n",
            "  \"HasAns_exact\": 64.24044734389562,\n",
            "  \"HasAns_f1\": 79.99211122229185,\n",
            "  \"HasAns_total\": 21460\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "#CODE TO EVALUATE THE QUALITY OF THE MODEL'S ANSWERS\n",
        "with open('predictions.json', 'w') as json_file:\n",
        "    json.dump(final_predictions, json_file)\n",
        "\n",
        "if duorc == True:\n",
        "  exact_scores, f1_scores = ev.get_raw_scores_duorc(ground_truth, final_predictions)\n",
        "  print(ev.make_eval_dict(exact_scores, f1_scores))\n",
        "else:\n",
        "  !python3 evaluate.py ground_truth.json predictions.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}